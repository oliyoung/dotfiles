= Subway Digital Menu Board Integration - Solution Architecture
:project-name: Subway Digital Menu Board Integration
:togaf-adm-phase: Phase E: Opportunities & Solutions
:last-updated: 2026-01-13
:document-type: Solution Architecture Document
:status: Development
:version: 0.1
:domain: ProductCatalog
:programme-lead: TBD
:domain-architect: O.Young@otr.com.au
:architecture-definition: xref:architecture-definitions:subway_dmb.adoc[Architecture Definitions/Subway DMB Integration]
:doctype: article
:toc: left
:toclevels: 4
:sectanchors:
:sectlinks:
:sectnums:
:icons: font
:source-highlighter: highlightjs
:experimental:
:description:
:keywords:

<<<
== Purpose and Scope

=== Purpose

This Solution Architecture defines how the Subway Digital Menu Board pricing integration will be implemented - the specific technology products, configurations, interfaces, and deployment architecture that realise the xref:architecture-definitions:subway_dmb.adoc[Subway DMB Integration Architecture Definition].

This solution establishes the **first cross-cloud domain integration pattern** for OTR Group by deploying the xref:bounded-contexts:product_catalog.adoc[ProductCatalogService] bounded context, implementing cross-cloud data synchronisation (Azure ↔ AWS), and establishing patterns that will serve as the foundation for future product catalog consumers (mobile app, online ordering).

=== Vision

> Enable accurate, automated pricing display on Subway digital menu boards across all OTR-operated stores by establishing a cross-cloud product catalog integration that bridges back-office systems (D365 Commerce on Azure) with customer-facing digital experiences (AWS), creating a scalable foundation for future ecommerce product catalog capabilities.

=== Architectural Approach

This architecture establishes OTR's **first cross-cloud integration pattern** using the following approach:

* *xref:bounded-contexts:product_catalog.adoc[ProductCatalogService]* (NEW bounded context): Product catalog and pricing facade, ingests pricing from D365 Commerce via S3, provides RESTful API for DMB platform, AWS-native implementation (Lambda/ECS, RDS PostgreSQL, S3, API Gateway)
* *Cross-Cloud Data Pipeline* (NEW pattern): S3-based data contract for cross-cloud integration (Azure → AWS), hourly/daily scheduled sync, foundation for future back-office data integrations
* *Anti-Corruption Layer*: ProductCatalogService translates D365 Commerce product model into OTR domain language, protects ecommerce domain from back-office system changes
* *Conformist Pattern*: ProductCatalogService conforms to D365 Commerce data contract (external system we don't control)
* *Phased rollout*: Pilot stores validation before full rollout

=== Scope

==== In Scope

* Solution Building Blocks (SBBs) - specific technology products and versions
* Interface specifications (REST API, data formats)
* Data architecture and database schema specifications
* Deployment architecture and AWS infrastructure configuration
* Transition architecture with implementation phases
* Technical validation criteria and testing approach
* External system integration specifications (D365 Commerce, Menuzen/Amped Digital)
* Cross-cloud data synchronisation pipeline

==== Out of Scope

===== Covered in xref:architecture-definitions:subway_dmb.adoc[Architecture Definition]

* Business requirements and stakeholder analysis
* Architecture decisions and rationale (ADRs)
* Bounded context definitions and responsibilities
* High-level integration patterns (ACL, Conformist, BFF)
* Business capability mapping and value streams
* Architecture principles and governance

===== Covered in Other Documents

* Detailed design (class diagrams, sequence diagrams for all flows)
* Implementation code and unit tests
* Business case and return-on-investment analysis
* Operational run books and support procedures

<<<
=== Architecture Overview

[mermaid]
----
---
config:
  layout: elk
---
flowchart TB
    Customer(["Subway Store"]) -- Displays --> DMB["Digital Menu Board<br>Menuzen Platform"]
    DMB -- Queries Pricing API --> ProductCatalog["ProductCatalogService<br>(AWS)"]
    ProductCatalog <-- Queries Pricing --> RDS["RDS PostgreSQL<br>(AWS)"]
    D365["D365 Commerce<br>(Azure)"] -- Exports Pricing --> ADF["Azure Data Factory"]
    ADF -- Cross-Cloud Transfer --> S3["S3 Bucket<br>(AWS)"]
    S3 -- Triggers ETL --> ETL["ETL Pipeline<br>Glue/Lambda<br>(AWS)"]
    ETL -- Loads Pricing --> RDS

    ProductCatalog -.-> MobileApp["Future: Mobile App"]
    ProductCatalog -.-> SMGB["Future: SMGB Online"]

    Customer:::customer
    DMB:::presentation
    ProductCatalog:::newService
    RDS:::infrastructure
    D365:::external
    ADF:::external
    S3:::infrastructure
    ETL:::infrastructure
    MobileApp:::presentation
    SMGB:::presentation

    classDef newService fill:#4CAF50,stroke:#2E7D32,stroke-width:3px,color:#fff
    classDef external fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    classDef infrastructure fill:#2196F3,stroke:#0D47A1,stroke-width:2px,color:#fff
    classDef presentation fill:#9C27B0,stroke:#4A148C,stroke-width:2px,color:#fff
    classDef customer fill:#607D8B,stroke:#263238,stroke-width:2px,color:#fff
----

==== Key Architectural Patterns

[width="100%",cols="25%,35%,40%",options="header",]
|===
|Pattern |Application |Rationale

|*Anti-Corruption Layer (ACL)*
|ProductCatalogService → D365 Commerce
|Protects ecommerce domain (AWS) from D365 Commerce model changes, translates between D365 pricing structure and OTR domain language, enables independent evolution

|*Conformist*
|ProductCatalogService → D365 Commerce
|D365 Commerce is back-office system we don't control - we conform to their data contract and export format

|*Backend-for-Frontend (BFF)*
|ProductCatalogService
|Aggregates pricing queries, store-specific pricing resolution, and future product catalog operations for digital channel clients (DMB, mobile, web)

|*Cross-Cloud Integration via Data Contract*
|Azure (D365 Commerce) → AWS (ProductCatalogService)
|S3-based data contract provides clean separation between back-office domain (Azure) and ecommerce domain (AWS), loose coupling, each domain uses native cloud services

|*Scheduled Batch Sync*
|D365 Commerce → ProductCatalogService (hourly/daily)
|Simpler than real-time CDC, meets DMB refresh requirements, reduces cross-cloud integration complexity

|*Eventually Consistent Caching*
|ProductCatalogService
|RDS PostgreSQL caches pricing data for fast API queries, accepts eventual consistency (max 1-24 hour lag) for performance and resilience

|===

<<<
== Solution Approach

* Deploy xref:bounded-contexts:product_catalog.adoc[ProductCatalogService] as new bounded context microservice
* Implement S3-based cross-cloud data pipeline (Azure → AWS)
* Establish Anti-Corruption Layer for D365 Commerce integration
* Phased rollout with pilot store validation

=== Related Documentation

This solution architecture references detailed technical specifications:

==== Architecture Definition

* xref:architecture-definitions:subway_dmb.adoc[Subway DMB Integration - Architecture Definition] - Business requirements, architecture decisions (ADRs), bounded context definitions, integration patterns

==== Bounded Context Specifications

* xref:bounded-contexts:product_catalog.adoc[ProductCatalogService Bounded Context] - Pricing cache aggregate, ETL services, API specifications

==== Interface Specifications

Detailed API specifications available in specifications directory:

* `specifications/rest/product-catalog-service-v1.yaml` - OpenAPI 3.0 specification for `/pricing/v1` endpoints
* `specifications/data/d365-pricing-export-schema.json` - D365 Commerce pricing export data contract

==== External System Documentation

* https://docs.microsoft.com/en-us/dynamics365/commerce/[Microsoft Dynamics 365 Commerce Documentation]
* https://www.menuzen.com/[Menuzen Digital Menu Board Platform]
* Amped Digital (agency managing Menuzen platform integration)

<<<
== Executive Summary

=== Challenge

OTR-operated Subway stores use digital menu boards (DMBs) on the Menuzen platform (managed by Amped Digital agency) to display product menus and pricing. Currently, pricing data must be manually updated or managed through separate vendor processes, creating:

* *Manual pricing maintenance*: Pricing updates require manual intervention, increasing operational overhead and risk of pricing errors
* *No integration with D365 Commerce*: Pricing data managed in D365 Commerce (OTR's product system of record) cannot automatically flow to DMBs
* *Limited scalability*: No foundation for future product catalog consumers (mobile app, online ordering)

=== Solution

Establish xref:bounded-contexts:product_catalog.adoc[ProductCatalogService] as a new bounded context that:

==== Cross-Cloud Product Catalog Integration

* Ingests pricing data from D365 Commerce (Azure) via scheduled S3 exports
* Caches pricing in RDS PostgreSQL for fast API queries
* Provides RESTful API for Menuzen DMB platform
* Translates D365 Commerce product model into OTR domain language (Anti-Corruption Layer)

==== Foundation for Future Ecommerce Capabilities

* Establishes cross-cloud integration pattern (Azure back-office → AWS ecommerce)
* Creates product catalog API reusable by mobile app, online ordering, future channels
* Implements data contract approach for loose coupling between domains

==== Automated Pricing Synchronisation

* Hourly/daily scheduled pricing updates from D365 Commerce
* Automated validation and error handling
* Store-specific pricing with price group hierarchy

=== Key Architectural Significance

This initiative establishes foundational patterns for OTR's ecommerce platform:

* **First cross-cloud domain integration**: Bridges Azure (back-office) and AWS (ecommerce) with clean data contract
* **Product catalog foundation**: Creates standardised API for all future product catalog consumers
* **Anti-Corruption Layer pattern**: Protects ecommerce domain from back-office system changes
* **Scalable architecture**: Supports future expansion to other OTR brands (Oporto, Red Rooster, etc.)

=== Success Criteria

[width="100%",cols="40%,60%",options="header",]
|===
|Criterion |Target

|DMB pricing accuracy
|>99.5% match with D365 Commerce

|API response time
|p95 <200ms, p99 <500ms

|Data freshness
|Pricing synced within 24 hours of D365 Commerce update

|System uptime
|99.9% API availability

|Pilot store validation
|Zero pricing discrepancies, overnight sync success for 30 days

|Scalability
|Support 100+ stores, 5000+ SKUs, 500 API requests/minute

|===

=== Technical Constraints

* D365 Commerce data export schedule (hourly/daily, not real-time)
* Cross-cloud data transfer limitations (network bandwidth, latency)
* Menuzen DMB platform API requirements (OAuth 2.0, rate limiting)
* AWS region: Australia East (ap-southeast-2) for data residency

<<<
== Technical Architecture

=== Solution Building Blocks

==== AWS Infrastructure Components

[width="100%",cols="30%,20%,50%",options="header",]
|===
|Component |Technology |Purpose

|S3 Bucket
|AWS S3
|Landing zone for D365 Commerce pricing exports, cross-account access from Azure

|ETL Pipeline
|AWS Glue / Lambda
|Extract, transform, validate, and load pricing data from S3 to RDS

|Database
|RDS PostgreSQL
|Cached pricing data storage, multi-AZ deployment

|API Gateway
|AWS API Gateway
|REST API exposure, OAuth 2.0 authentication, rate limiting

|API Service
|ECS Fargate / Lambda
|ProductCatalogService API implementation

|Caching (Optional)
|Redis / ElastiCache
|API response caching for high-traffic queries

|Monitoring
|CloudWatch
|Metrics, logs, alarms, dashboards

|Alerting
|SNS
|Failure notifications (ETL failures, API degradation)

|===

=== Deployment Architecture

==== AWS Infrastructure Diagram

[mermaid]
----
graph TB
    subgraph "AWS Region: ap-southeast-2"
        subgraph "VPC: otr-ecommerce"
            subgraph "Public Subnet"
                ALB[Application Load Balancer]
                NAT[NAT Gateway]
            end

            subgraph "Private Subnet A"
                ECS1[ECS Task<br/>API Container 1]
                RDS1[(RDS Primary<br/>PostgreSQL)]
            end

            subgraph "Private Subnet B"
                ECS2[ECS Task<br/>API Container 2]
                RDS2[(RDS Standby<br/>Read Replica)]
            end

            subgraph "Isolated Subnet"
                GLUE[Glue Job<br/>ETL Pipeline]
            end
        end

        S3[S3 Bucket<br/>Pricing Exports]
        API[API Gateway]

        S3 --> GLUE
        GLUE --> RDS1
        RDS1 -.->|Replication| RDS2
        API --> ALB
        ALB --> ECS1
        ALB --> ECS2
        ECS1 --> RDS1
        ECS2 --> RDS2
    end

    EXT[Menuzen Platform] -->|HTTPS| API

    AZURE[Azure Back-Office] -.->|Cross-Cloud Export| S3
----

<<<
== Transition Architecture

=== Transition Approach

Phased rollout with pilot store validation before full deployment

=== Transition States

==== TS-0: Foundation (Weeks 1-2)

===== Activities

* Define data contract with back-office team
* Set up S3 bucket with cross-account IAM access (Azure → AWS)
* Provision RDS PostgreSQL instance
* Create database schema

===== Deliverables

* Data contract specification document
* S3 bucket configured with cross-account access
* RDS instance provisioned and secured
* Database schema DDL scripts

==== TS-1: Data Pipeline Development (Weeks 2-4)

===== Activities

* Build ETL pipeline (Glue/Lambda)
* Implement data validation logic
* Set up error handling and alerting (SNS, CloudWatch)
* Test with sample data from back-office

===== Deliverables

* ETL pipeline code (Glue job or Lambda function)
* Data validation rules implemented
* CloudWatch alarms configured
* SNS notification setup for failures

==== TS-2: API Development (Weeks 4-6)

===== Activities

* Build REST API service (ECS container)
* Implement OAuth 2.0 authentication
* Add caching layer (API Gateway cache or ElastiCache)
* API documentation (OpenAPI spec)

===== Deliverables

* API service containerized and deployed to ECS
* OAuth 2.0 authentication implemented
* API Gateway configured with caching
* OpenAPI specification document

==== TS-3: Pilot Stores (Weeks 6-7)

===== Activities

* Integrate with Menuzen platform (provide credentials via Amped Digital)
* Deploy to 2-3 pilot Subway stores
* End-to-end testing with production data
* Performance testing (load, stress)
* Security testing (penetration test)
* Monitor pilot stores for 30 days

===== Deliverables

* Menuzen platform credentials provisioned (via Amped Digital)
* Pilot stores displaying accurate pricing
* Test results documentation
* Performance test report
* Security test report
* 30-day pilot validation report

===== Exit Criteria

* Zero pricing discrepancies at pilot stores
* Overnight sync completes within 1 hour
* p95 API response time <200ms
* No P1/P2 incidents during pilot period

==== TS-4: Full Rollout (Week 8+)

===== Activities

* Deploy to all Subway stores
* Monitor for 48 hours
* Knowledge transfer to ecommerce operations team
* Document runbooks

===== Deliverables

* Production deployment complete
* All stores enabled
* Monitoring dashboards configured
* Operations runbooks documented
* Team training completed

===== Success Criteria

* >99% sync success rate
* <5 manual pricing corrections per month
* No P1/P2 incidents
* API ready for future consumers (mobile app, SMGB Online)

=== Rollback Plan

==== Rollback Triggers

* Pilot stores experience >5% pricing discrepancies
* API performance degradation (p95 >1 second)
* Critical ETL failures preventing pricing updates

==== Rollback Procedure

. Vendor reverts to previous manual DMB pricing process
. Investigate and fix issues in non-production environment
. Schedule new cutover date after fixes verified

<<<
== Non-Functional Requirements

=== Performance Requirements

[width="100%",cols="40%,60%",options="header",]
|===
|Requirement |Target

|API response time
|p95 <200ms, p99 <500ms

|ETL completion time
|<1 hour for overnight sync

|Database query performance
|<100ms for typical pricing queries

|===

=== Availability Requirements

[width="100%",cols="40%,60%",options="header",]
|===
|Requirement |Target

|System uptime
|99.9% (API availability)

|RDS availability
|Multi-AZ deployment for failover

|===

=== Scalability Requirements

[width="100%",cols="40%,60%",options="header",]
|===
|Requirement |Target

|Store capacity
|Support 100+ stores, 5000+ SKUs

|API throughput
|500 requests/minute

|Concurrent connections
|50 simultaneous API clients

|===

=== Security Requirements

[width="100%",cols="40%,60%",options="header",]
|===
|Requirement |Target

|Authentication
|OAuth 2.0 Client Credentials flow

|Encryption in transit
|TLS 1.2+ for all API calls

|Encryption at rest
|S3 SSE-S3, RDS encryption enabled

|Cross-cloud access
|IAM roles with least privilege

|===

=== Data Quality Requirements

[width="100%",cols="40%,60%",options="header",]
|===
|Requirement |Target

|Pricing accuracy
|>99.5% match with D365 Commerce

|Data freshness
|Pricing synced within 24 hours of D365 update

|===

<<<
== Monitoring and Observability

=== Key Metrics

* API requests per minute
* API latency (p50, p95, p99)
* API error rate (4xx, 5xx)
* Cache hit ratio
* RDS CPU/memory utilisation
* RDS connections
* ETL job execution time
* Records processed per ETL run
* Data validation failures
* S3 file size trends

=== Alerting Thresholds

[width="100%",cols="50%,15%,35%",options="header",]
|===
|Alert Condition |Priority |Action

|API error rate >1% for 5 minutes
|P1
|Page on-call engineer immediately

|API latency p95 >1s for 5 minutes
|P2
|Notify operations team

|ETL job failure
|P1
|Page on-call engineer + notify back-office team

|RDS CPU >80% for 10 minutes
|P2
|Notify operations team

|No new data in S3 for 90 minutes
|P2
|Notify back-office team

|===

=== Dashboards

* Real-time API metrics (requests, latency, errors)
* ETL pipeline health (success rate, execution time, validation failures)
* Infrastructure health (RDS, ECS, S3)
* Business metrics (stores served, products synchronized, pricing updates)

<<<
== Assumptions and Constraints

=== Assumptions

* D365 Commerce provides pricing exports on agreed schedule (hourly/daily)
* Back-office team maintains data contract stability
* Menuzen DMB platform supports OAuth 2.0 authentication
* AWS Australia East region meets data residency requirements

=== Constraints

* D365 Commerce data export schedule (not real-time)
* Cross-cloud data transfer latency (Azure → AWS)
* Menuzen platform API rate limiting requirements
* AWS region: ap-southeast-2 (Sydney) for data residency

<<<
== Appendices

=== Appendix A: API Specification

*Endpoint:* `GET /api/v1/pricing`

*Authentication:* OAuth 2.0 Bearer Token (Client Credentials flow)

*Response Format:*

[source,json]
----
{
  "generated_at": "2025-01-13T10:30:00Z",
  "products": [
    {
      "product_number": "398602",
      "product_name": "Italian BMT Footlong",
      "price": [
        {
          "account_relation": "POS-AU-NAT",
          "amount": "14.40",
          "stores": [19598, 20260, 20334]
        }
      ]
    }
  ]
}
----

*Response Codes:*

* 200: Success
* 401: Unauthorised (invalid/expired token)
* 429: Too Many Requests (rate limit exceeded)
* 500: Internal Server Error
* 503: Service Unavailable

*Rate Limiting:* 100 requests per hour per client

=== Appendix B: Data Contract Details

*S3 Bucket Configuration:*

* *Bucket Name:* `otr-pricing-exports-prod`
* *Region:* `ap-southeast-2` (Sydney)
* *Path Pattern:* `/subway/YYYY/MM/DD/pricing_YYYYMMDDHHmmss.parquet`
* *File Format:* Apache Parquet
* *Compression:* Snappy
* *Encryption:* SSE-S3

=== Appendix C: Operational Runbooks

==== Runbook: ETL Failure

*Symptom:* ETL job fails, no new pricing data loaded

*Investigation Steps:*

. Check CloudWatch Logs for ETL job error details
. Verify S3 file exists at expected path
. Verify file format and schema validity
. Check RDS connectivity from Glue/Lambda
. Review IAM role permissions

*Resolution:*

* If transient failure (network, timeout): Retry job manually
* If data quality issue: Contact back-office team
* If schema change: Update ETL transformation logic
* If RDS connectivity: Check security groups, network ACLs

*Escalation:* Back-office team if data source issue; Infrastructure team if AWS service issue

==== Runbook: API High Latency

*Symptom:* API response time >1 second p95

*Investigation Steps:*

. Check CloudWatch metrics for RDS CPU/memory
. Check API Gateway metrics for integration latency
. Review RDS slow query logs
. Check cache hit rate
. Review ECS task metrics

*Resolution:*

* If RDS constrained: Scale instance vertically or add read replicas
* If slow queries: Add indexes, optimise queries
* If cache ineffective: Increase cache TTL or implement ElastiCache
* If ECS constrained: Scale out tasks

*Escalation:* Database team if persistent query performance issues

<<<
== Document Control

[cols="1,2,2,4"]
|===
|Version |Date |Author |Changes

|0.1
|2026-01-13
|O.Young (Domain Architect, Ecommerce)
|Initial draft - aligned with Salesforce solution architecture structure

|===

*Approval Required From:*

* [ ] Development Lead (confirms implementation approach)
* [ ] DevOps Lead (confirms infrastructure design)
* [ ] Operations Lead (confirms runbooks and monitoring)
* [ ] Product Owner (confirms delivery timeline)

